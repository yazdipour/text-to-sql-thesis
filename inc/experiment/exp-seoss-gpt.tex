\subsection{SEOSS evaluation with GPT 3.5 and GPT 4}

Here we discuss our experience using the ChatGPT API for the Text-to-SQL task on the SEOSS dataset. We will provide a brief overview of the GPT architecture and delve into the specifics of ChatGPT, emphasizing its capabilities and potential for addressing this particular challenge.
\subsubsection{GPT Architecture}
Generative Pre-trained Transformers (GPT) \cite{radford2018improving} are cutting-edge language models based on the Transformer architecture, which has had a significant impact on the field of natural language processing. The Transformer architecture is characterized by its use of self-attention mechanisms, enabling parallel processing of sequences and resulting in more efficient training and improved performance across various NLP tasks.
GPT models are generative in nature, designed to produce text based on the provided context. They are pre-trained on massive amounts of text data, allowing them to learn the inherent structure and patterns in natural language. The pre-training phase involves unsupervised learning using a masked language modeling objective. After pre-training, GPT can be fine-tuned for specific tasks, such as translation, summarization, or, as in our case, Text-to-SQL.

\subsubsection{ChatGPT}

ChatGPT and GPT-4 models represent an evolution in language models tailored explicitly for conversational interfaces. These models exhibit a distinct behavior compared to older GPT-3 models. While previous models operated on a text-in and text-out basis (accepting a prompt string and returning a completion to append to the prompt), ChatGPT and GPT-4 models follow a conversation-in and message-out approach. They expect input formatted in a chat-like transcript format and return a completion representing a model-generated message within the chat. This format is designed for multi-turn conversations but can also be effective in non-chat scenarios.
For our experiment with the SEOSS dataset, we utilized the ChatGPT API to submit natural language questions and retrieve generated SQL queries. The API allowed for the seamless integration of ChatGPT into our workflow and provided an efficient and effective means to process the dataset.
Throughout the evaluation, we observed that ChatGPT successfully generated accurate and syntactically correct SQL queries for a wide range of questions. The model excelled at handling complex queries and demonstrated a deep understanding of the underlying database schema.
However, there were instances where ChatGPT generated SQL queries that deviated from the desired output, particularly in edge cases and questions with ambiguous semantics. To mitigate this issue, we employed a combination of custom LLM prompts to ensure the generated queries met the required quality standards.

\subsubsection{ChatGPT Prompts and Roles}

ChatGPT utilizes prompts to guide its response generation process. Prompts are input messages with an associated role, which can be one of three types: system, user, or assistant. The role of the author of a message provides context for the conversation and affects the model's response.
System: A high-level instruction for the conversation, usually used to provide guidance or set the context for the assistant.
User: A message provided by the user, typically in the form of a question or a request for the assistant to process.
Assistant: The response generated by the assistant is based on the context provided by the system and user messages.
The assistant processes the messages in the order they appear in the list and responds accordingly. Using system instructions can be an effective way to guide the assistant's behavior throughout the conversation.
In our Text-to-SQL task on the SEOSS dataset, we used the following prompt structure:

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Role} & \textbf{Content}                                                                             \\ \hline
        System        & You are a helpful assistant for generating syntactically correct read-only                   \\
                      & SQL to answer a given question.                                                              \\
                      & Database: \$dbname                                                                           \\
                      & The following are tables you can query:                                                      \\
                      & ---------------------                                                                        \\
                      & \$schemas                                                                                    \\
                      & ---------------------                                                                        \\
                      & Do not use IN keyword.                                                                       \\
                      & If it is necessary to use AS then use it like T1 T2 ..., but if the alias                    \\
                      & name is not going to be used in query again, then do not use.                                \\
                      & Do not filter WHERE for being NOT NULL if it is not necessary.                               \\
                      & \small{If in using $COUNT(*) and COUNT(COLUMN)$ there is no difference then use $COUNT(*)$.} \\
                      & Instead of $<>$ use $!=$.                                                                    \\
                      & Write one valid SQL in markdown format.                                                      \\ \hline
        User          & Generate syntactically correct read-only SQL to answer the following                         \\
                      & question: \$question                                                                         \\ \hline
    \end{tabular}
    \caption{Prompt structure used in the Text-to-SQL task on the SEOSS dataset}
\end{table}

In this prompt, we first provided a system message that set the context for the system, including information about the database, tables, and specific syntax requirements for the generated SQL query. The user message then contained the natural language question for which the assistant was expected to generate the corresponding SQL query.
Using this prompt structure allowed us to effectively communicate the task requirements and constraints to ChatGPT, resulting in more accurate and syntactically correct SQL query generation.


\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        \multirow{2}*{Exact Match Accuracy} & easy  & medium & hard  & extra hard & all   \\
                                            & 392   & 378    & 77    & 84         & 931   \\ \hline
        SQLNet                              & 0.023 & 0.000  & 0.000 & 0.000      & 0.010 \\ \hline
        RatSQL + Glove                      & 0.309 & 0.214  & 0.091 & 0.000      & 0.224 \\ \hline
        RatSQL + Bert                       & 0.161 & 0.201  & 0.065 & 0.012      & 0.156 \\ \hline
        PICARD + T5Base + 4Beam             & 0.446 & 0.254  & 0.182 & 0.012      & 0.307 \\ \hline
        PICARD + T5Large + 4Beam            & 0.571 & 0.410  & 0.182 & 0.060      & 0.427 \\ \hline
        GPT 3.5-turbo                       & 0.551 & 0.460  & 0.130 & 0.190      & 0.447 \\ \hline
        GPT 4                               & 0.709 & 0.505  & 0.104 & 0.131      & 0.524 \\ \hline
    \end{tabular}
    \caption{Comparison between Exact Match Accuracy}
\end{table}

The table presents a comparison of the exact match accuracy for various models that have not been fine-tuned for our dataset. These models are assessed across five difficulty levels: easy, medium, hard, extra hard, and all. The large GPT models demonstrated the highest accuracy across all levels. Upon investigating the reasoning behind the lower accuracy on the hard level, we discovered that the model occasionally generated correct but complex queries, which led to confusion in our evaluation method.

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|L|L|}
        \hline
        Model                    & \textbf{Execution Accuracy} & \textbf{Time} & \textbf{Cost}  \\ \hline
        PICARD + T5Base + 4Beam  & 0.307                       & ~400min       & Local Hardware \\ \hline
        PICARD + T5Large + 4Beam & 0.427                       & ~720min       & Local Hardware \\ \hline
        GPT 3.5-turbo            & 0.447                       & 37min         & \$2            \\ \hline
        GPT 4                    & 0.524                       & 78min         & \$10           \\ \hline
    \end{tabular}
    \caption{Expermiment Accuracy vs Resources used}
\end{table}
