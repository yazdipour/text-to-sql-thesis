\subsection{SEOSS evaluation with GPT 3.5 and GPT 4}

\subsubsection{GPT}

\subsubsection{ChatGPT}

\subsubsection*{Prompt}

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        \multirow{2}*{Exact Match Accuracy} & easy  & medium & hard  & extra hard & all   \\
                                            & 392   & 378    & 77    & 84         & 931   \\ \hline
        SQLNet                              & 0.023 & 0.000  & 0.000 & 0.000      & 0.010 \\ \hline
        RatSQL + Glove                      & 0.309 & 0.214  & 0.091 & 0.000      & 0.224 \\ \hline
        RatSQL + Bert                       & 0.161 & 0.201  & 0.065 & 0.012      & 0.156 \\ \hline
        PICARD + T5Base + 4Beam             & 0.446 & 0.254  & 0.182 & 0.012      & 0.307 \\ \hline
        PICARD + T5Large + 4Beam            & 0.571 & 0.410  & 0.182 & 0.060      & 0.427 \\ \hline
        GPT 3.5-turbo                       & 0.551 & 0.460  & 0.130 & 0.190      & 0.447 \\ \hline
        GPT 4                               & 0.709 & 0.505  & 0.104 & 0.131      & 0.524 \\ \hline
    \end{tabular}
    \caption{Comparison between Exact Match Accuracy}
\end{table}


The table compares the exact match accuracy of various models that are not fine-tuned for our dataset. The models are evaluated on five difficulty levels: easy, medium, hard, extra hard, and all.

Overall table:

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|L|L|}
        \hline
        Model & Version   & \textbf{Execution Accuracy} & \textbf{Time} & \textbf{Money} \\ \hline
        GPT   & 3.5-turbo & 0.423                       & 447min        & \$0            \\ \hline
        GPT   & 3.5-turbo & 0.423                       & 447min        & \$20           \\ \hline
    \end{tabular}
    \caption{Expermiment Accuracy vs Resources used}
\end{table}