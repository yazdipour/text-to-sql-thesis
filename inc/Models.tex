\section{State-of-the-art Text-To-SQL solutions}

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.99\textwidth]{pics/Timeline.png}
    \caption{An overview of the deep learning process for Text-to-SQL.}
    \label{fig:timeline}
\end{figure}

An efficient text-to-SQL solution requires state-of-the-art natural language processing techniques.
As a result of the neural network's ability to handle only numerical inputs and not raw text, word embedding has been used to represent numerical words.

Aside from that, in the past few years, language models have become increasingly popular as a solution for increasing performance in natural language processing tasks.

Assuming that words have numerical representations that differ from those of other words, word embeddings aim to map each word to a multidimensional vector, incorporating valuable information about the word. In addition to the brute-force creation of one-hot embeddings, researchers have developed highly efficient methods for creating representations that convey a word's meaning and relationships with other words. In most, if not all, Text-to-SQL systems, word embedding techniques such as Word2Vec\cite{DBLP:journals/corr/Rong14}, GloVe, and WordPiece embeddings\cite{DBLP:journals/corr/WuSCLNMKCGMKSJL16} are used.

Recently Language models have been shown to excel at NL tasks as a new type of pre-trained neural network. It is important to note that language models are not a replacement for word embeddings since they are neural networks and need a way to transform words into vectors.

Depending on the specific problem they want to solve, researchers can adapt the pre-trained model's inputs and outputs and train it for an additional number of epochs on their dataset. Thus, we can achieve state-of-the-art performance without complex architectures \cite{DBLP:journals/corr/abs-1810-04805}. Recent neural network architectures, like the Transformer\cite{DBLP:journals/corr/VaswaniSPUJGKP17}, have been used to achieve such performance by these models, which excel at handling NL and sequences of NL that are characterized by connections between words. Several language models have been used to handle the text-to-SQL task, including BERT \cite{DBLP:journals/corr/abs-1810-04805} and MT-DNN \cite{DBLP:journals/corr/abs-1901-11504}, while new models pre-trained specifically for structured data tasks are emerging, such as TaBERT\cite{DBLP:journals/corr/abs-2005-08314} and GraPPa \cite{DBLP:journals/corr/abs-2009-13845}.

% Furthermore, Text-to-SQL models and techniques represented in the SPIDER challenge will be studied and evaluated. 
The research section will assess the best state-of-the-art research in this field, starting from Seq2SQL\cite{zhong_seq2sql_2017} study in 2017 with the hype in WikiSQL challange and we will continue with SQLova\cite{hwang_comprehensive_2019} SQLNet\cite{xu_sqlnet_2017} and indtroduction of transformers and BERT with focus on RAT-SQL\cite{wang_rat-sql_2021} (2019), BRIDGE\cite{lin_bridging_2020} with BERT, HydraNet\cite{lyu_hybrid_2020} (2020), and the most recent solution with Google T5\cite{raffel_exploring_2020}, PICARD\cite{scholak_picard_2021} in SPIDER (2021).

% ////////////////////////////////////////////////////////////////////////////////

After reviewing the research papers on these models, we will study the implementation steps of these models. Moreover, evaluation methods and approaches to compare these models in accuracy for different datasets and if they are usable and reliable enough for our usage.

Most of these studies have excellent documentation regarding their implantation. Execution of these studies will be documented and published on Github. Nonetheless, In case of old and impractical implementation instructions, we will skip the implementation and continue with the top models available.

\input{inc/models/Seq2sql}
\input{inc/models/SQLNet}
\input{inc/models/SyntaxSQLNet}
\input{inc/models/GrammarSQL}
\input{inc/models/IRNet}
\input{inc/models/EditSQL}
\input{inc/models/RAT-SQL}
\input{inc/models/BRIDGE}
\input{inc/models/Picard}