\subsection{Terminology}

Here is an updated list of key terminology and vocabulary that you may need to know before studying Text-to-SQL language models:

% \subsubsection{SQL (Structured Query Language)} The standard, widely used programming language designed to manage relational databases, which enables users to store, retrieve and manipulate data.
\subsubsection{Natural Language Processing (NLP)}

The field of study focused on the interaction between human language and computers, which ranges from understanding spoken language to generating natural language text.

\subsubsection{Tokenization}

The process of breaking up a sentence into individual words or phrases is necessary for tasks such as machine translation and text summarization.

\subsubsection{WordPiece embeddings}

WordPiece\cite{DBLP:journals/corr/WuSCLNMKCGMKSJL16} is a tokenization approach used in natural language processing (NLP) to break down words into smaller units, also known as pieces. It is an extension of the original word2vec parameter learning algorithm and is used to address out-of-vocabulary (OOV) words, which are words that did not appear in the training data.
This technique divides each word into a series of subword units learned during the training phase based on their frequency and consistency within words. These subword units are stored in a shared vocabulary, dubbed the WordPiece vocabulary, and can be used for multiple words.
This system can represent rare or unseen words as a combination of more common subword units, which are more likely to be in the vocabulary. As a result, the model can handle OOV words more efficiently and reduce the vocabulary size, leading to a more economical representation of the language.
In NLP models, words are usually portrayed as dense vectors referred to as word embeddings. WordPiece embeddings extend this representation by breaking words down into subword units and representing each piece as a dense vector. These subword embeddings are then combined to represent the whole word.
The use of WordPiece embeddings has various advantages in NLP models. Firstly, it enables the model to treat OOV words more effectively by representing them as a combination of more common subword units. Secondly, it decreases the vocabulary size, resulting in a more succinct representation of the language. Finally, it enhances the model's capability to learn fine-grained representations of words and their meanings, resulting in improved performance in NLP tasks.

\subsubsection{Word2Vec}

Word2Vec\cite{DBLP:journals/corr/Rong14} is a well-known word embedding approach in NLP that encodes words as dense vectors in an unending, high-dimensional area. This technique is designed to capture the significance and context of words, providing an improved representation of words compared to classic one-hot encoding.
The fundamental concept behind Word2Vec is to train a neural network to anticipate the context words about a target word, given the target word. As the model is trained, the weights of the neural network are adjusted in such a way that the dot product of the input layer (representing the target word) and the output layer (representing the context words) closely estimate the probability distribution of the context words given the target word.
Word2Vec can be trained to utilize two different algorithms: Continuous Bag-of-Words (CBOW) and Skip-gram. CBOW predicts the target word given the context words, while Skip-gram predicts the context words given the target word. The algorithm selection relies on the particular NLP task and the data available for training.

\subsubsection{Encoder-Decoder Architecture}

A powerful neural network architecture that utilizes an encoder\cite{cho-etal-2014-learning} to transform the input data into a compact and meaningful representation and a decoder to generate the desired output from that representation. This architecture has been widely used in many applications such as language translation, image captioning, and text summarization to produce high-quality results. Furthermore, the encoder-decoder architecture has the advantage of being able to learn complex relationships between input and output, making it a suitable tool for many challenging tasks.

\subsubsection{Transformers}

\input{inc/models/transformers.tex}

\subsubsection{Self-attention}

Self-attention\cite{https://doi.org/10.48550/arxiv.1706.03762} is a mechanism used in the transformer architecture that enables the model to identify the significance of different elements of the input sequence, so as to be able to generate an output that is more accurate and effective. This mechanism allows the model to take into account the relationships between different parts of the input sequence and to factor those relationships into its output. Additionally, self-attention allows the model to capture patterns from the input sequence and to use those patterns to generate more meaningful output. It is this combination of factors that makes self-attention such an important tool for deep learning models.

\subsubsection{Pre-training and Fine-tuning}

Pre-training refers to training a model on a large dataset and then fine-tuning it on a smaller dataset for a specific task, which helps to improve the model's performance on the specific task.

\subsubsection{Long Short-Term Memory (LSTM)}

A type of recurrent neural network that has been designed to store information over a longer period of time than traditional neural networks, allowing it to better capture long-term dependencies\cite{Hochreiter1997LongSM}.
This makes it especially well-suited for tasks such as language modeling and text generation, where it can take into account the context of the text in order to generate more accurate outputs.
In addition, LSTM networks are able to identify patterns in the data that would be difficult for traditional networks to capture. This makes them ideal for tasks such as sequence prediction and classification, where they can identify patterns that would otherwise be too subtle for traditional networks to detect.

\subsubsection{Bidirectional Encoder Representations from Transformers(BERT)}

A pre-trained Transformer model that has been trained on a large corpus of text, with the primary aim of pre-training language representations for use in natural language processing tasks\cite{devlin-etal-2019-bert}. This pre-training helps to give BERT a strong understanding of the language structure and helps in faster training times for downstream tasks. BERT can be fine-tuned for various applications, such as Text-to-SQL, where it can provide better performance than non-specialized models. By leveraging the already learned representations from the pre-trained model, BERT is able to quickly adjust to the task at hand, resulting in faster training times.

\subsubsection{SQL Constructs}

The elements of SQL language such as SELECT, FROM, WHERE, JOIN, are used to build queries and retrieve data from a database.

\subsubsection{Evaluation Metrics}

Measures used to evaluate the performance of Text-to-SQL models, such as accuracy, F1-score, and Exact Match score, are used to compare different models and determine the best-performing model.

\subsubsection{Baseline Model}

A model that serves as a reference point or starting point for comparison, providing a baseline for performance against which other models can be evaluated.

\subsubsection{Incremental decoding}

A decoding strategy where the model generates a sequence of tokens one at a time, at each step conditioned on the previous tokens, the input, and the context of the sentence. This approach allows for a more dynamic and flexible generation of output, as it takes into account a variety of factors when making decisions about the next token. This strategy also helps the model avoid repeating itself, providing more diverse and unique outputs. Furthermore, incremental decoding helps the model to better capture the nuance of the language as it is able to build upon previous decisions and refine its output as it progresses\cite{huang-mi-2010-efficient}.

\subsubsection{Semantic parsing}

Semantic parsing\cite{krishnamurthy-etal-2017-neural} is an area of natural language processing that involves extracting the meaning or intent from text. One type of Semantic Parsing, Text-to-SQL, involves the conversion of natural language problems into SQL query statements. This is a challenging task, one that requires the use of advanced machine learning and natural language processing algorithms. As such, the research conducted in this field seeks to explore the various solutions and practices that have been employed by researchers in order to effectively tackle this problem. Furthermore, it is also important to note that this problem is not just limited to the conversion of natural language into SQL query statements, as there are other applications of Semantic Parsing that have been explored, such as Natural Language Generation (NLG). Overall, by understanding the various techniques used for Semantic Parsing, we can gain a better understanding of the complexities involved in this task and how best to approach it.