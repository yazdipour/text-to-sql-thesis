\section{Benchmark Dataset}

Datasets play a crucial role in developing and evaluating Text-to-SQL models for semantic parsing of natural language phrases. A variety of benchmark datasets are available, each with unique characteristics and features. Examples of early datasets include ATIS\cite{dahl-etal-1994-expanding}, GeoQuery\cite{10.1007/3-540-44795-4_40}, and Yelp\cite{10.1145/3133887}, which focus on a single topic and database. More recent datasets, such as WikiSQL\cite{zhong_seq2sql_2017} and Spider\cite{yu_spider_2019}, are larger and cover a broader range of domains.

Additionally, new datasets include more advanced queries to assess the generalization capabilities of models. These benchmark datasets provide a standardized testbed for evaluating the performance of Text-to-SQL models and are widely used in the research community. They vary in complexity, size, and annotation, allowing researchers to evaluate models' performance at different levels and under different scenarios. This chapter will review the top benchmark datasets used in the Text-to-SQL Semantic Parsing community and discuss their significance for the research community.

\subsection{Single-Domain}

\subsubsection{ATIS (Air Travel Information System) and GeoQuery}

ATIS (Air Travel Information System)\cite{dahl-etal-1994-expanding} and GeoQuery\cite{10.1007/3-540-44795-4_40} are two datasets that are frequently utilized for semantic parsing, a technique for converting natural language inquiries into a structured meaning representation. The ATIS dataset consists of audio recordings and hand transcripts of individuals using automated travel inquiry systems to search for information regarding flights. It is structured using a relational schema to organize data from the official airline guide, with 25 tables containing information concerning fares, airlines, flights, cities, airports, and ground services.
All questions concerning this dataset can be answered using a single relational query. This makes it an ideal choice for training deep learning models, as it is designed for a specific domain and the queries are relatively straightforward.

Furthermore, the questions in the ATIS dataset are mainly limited to select and project queries. On the other hand, GeoQuery is made up of seven tables from the US geography database and 880 natural languages to SQL pairings. It includes geographic and topographical characteristics such as capitals, populations, and landforms. While both datasets are regularly employed to train deep learning models, GeoQuery is more comprehensive and provides a wider range of queries than ATIS. This includes join and nested queries, as well as grouping and ordering queries, which are absent in the ATIS dataset. As a result, GeoQuery is better equipped to answer more complex queries, making it a better choice for training AI models.

% A relational schema is used to organize data from the official airline guide in the ATIS corpus. There are 25 tables containing information about fares, airlines, flights, cities, airports, and ground services. All questions related to this dataset can be answered using a single relational query. The relational database uses shorter tables for this dataset to answer queries intuitively.

% Here is an example query from the ATIS dataset: Input is in natural language, and the output is in \lambda calculus.

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=0.8\textwidth]{pics/db/ATIS.png}
%     \caption{Example from ATIS dataset for semantic parsing}
%     \label{fig:ATIS}
% \end{figure}

% \subsubsection{GeoQuery Dataset}

% United States geography is represented in the Geoquery dataset. About 800 facts are expressed in Prolog. State, city, river, and mountain information can be found in the database. Geographic and topographical attributes such as capitals and populations make up the majority of the attributes.

\subsubsection{IMDb Dataset}

The IMDb dataset is a well-known dataset in the machine learning community. It contains 50,000 reviews from IMDb and has a limit of 30 reviews per movie\cite{maas-EtAl:2011:ACL-HLT2011}. It is noteworthy that the dataset is balanced in terms of positive and negative reviews, which are equally represented. When creating the dataset, reviews with a score of 4 out of 10 were considered negative and those with a score of 7 out of 10 were considered positive. Neural reviews were excluded to maintain the quality of the dataset. The dataset is divided into training and testing datasets, each with an equal portion. To ensure fairness and accuracy in the results, the dataset creators have taken special care to keep the training and testing datasets balanced.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.8\textwidth]{pics/db/IMDb.png}
    \caption{Database Structure of IMDb dataset}
    \label{fig:IMDb}
\end{figure}

% \newpage % to avoid page break

\subsubsection{Advising Dataset}

The Advising dataset\cite{finegan-dollak-etal-2018-improving} was created in order to propose improvements in text2SQL systems. The creators of the dataset compare human-generated and automatically generated questions, citing properties of queries that relate to real-world applications. The dataset consists of questions from university students about courses that lead to particularly complex queries. The data is obtained from a fictional student database which includes student profile information such as recommended courses, grades, and previous courses. Moreover, in order to obtain the data for the dataset, academic advising meetings were conducted where students were asked to formulate questions they would ask if they knew the database. After obtaining the questions, the creators of the dataset compared the query results with those from other datasets such as ATIS, GeoQuery, and Scholar. It is evident that many of the queries in the Advising dataset were the same as those found in the other datasets.

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=0.5\textwidth]{pics/db/Advising.png}
%     \caption{Example from Advising dataset \cite{vig_comparison_2019}}
%     \label{fig:Advising}
% \end{figure}

\subsubsection{MAS (Microsoft Academic Search)}

MAS, or Microsoft Academic Search\cite{roy2013the}, is a database of academic and social networks and a collection of queries. It has a total of 17 tables in its database, as well as 196 natural languages to SQL pairs. MAS can handle join, grouping, and nested queries but does not support ordering queries.

There are a few limitations to be aware of when using natural language queries within MAS. Firstly, all-natural language questions must begin with the phrase ”return me” and can not include an interrogative statement or a collection of keywords. Additionally, all queries must follow the proper grammatical conventions.

\clearpage

\subsection{Large Scale Cross-Domain}

\input{inc/datasets/wikisql}
\input{inc/datasets/spider}
\input{inc/datasets/sede}
\input{inc/datasets/seoss}

In this chapter, we have reviewed various datasets widely used in the Text-to-SQL Semantic Parsing community. These datasets vary in complexity, size, and annotation, providing a standardized testbed for evaluating the performance of Text-to-SQL models. We have discussed their unique characteristics and features from early datasets such as ATIS and GeoQuery to more recent datasets such as WikiSQL and Spider.
The datasets discussed in this chapter are a valuable resource for the research community to evaluate the progress and performance of Text-to-SQL models. The continued development and improvement of these datasets will be necessary for advancing the field of Text-to-SQL Semantic Parsing.
The table\ref{tab:datasets} below provides an overview of the datasets mentioned in this chapter, including the number of queries and questions sorted by year.

\begin{table}[!ht]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        \textbf{Dataset} & \textbf{Year} & \textbf{Tables} & \textbf{Questions} & \textbf{Unique Queries} & \textbf{Domain}            \\ \hline
        ATIS
                         & 1994          & 25              & 5280               & 947                     & Air Travel Information     \\ \hline
        GeoQuery
                         & 2001          & 8               & 877                & 246                     & US geography database      \\ \hline
        Academic
                         & 2014          & 15              & 196                & 185                     & Microsoft Academic Search  \\ \hline
        IMDB
                         & 2015          & 16              & 131                & 89                      & Internet Movie Database    \\ \hline
        Scholar
                         & 2017          & 7               & 817                & 193                     & Academic Publications      \\ \hline
        Yelp
                         & 2017          & 7               & 128                & 110                     & Yelp Movie Website         \\ \hline
        WikiSQL
                         & 2017          & 24,241          & 80,654             & 77,840                  & Wikipedia                  \\ \hline
        Advising
                         & 2018          & 10              & 4570               & 211                     & Student Course Information \\ \hline
        Spider
                         & 2018          & 645             & 10,181             & 5693                    & 138 Different Domains      \\ \hline
        SEDE
                         & 2021          & 29              & 12,023             & 11,767                  & Stack Exchange             \\ \hline
        SEOSS
                         & 2022          & 13              & 1,162              & 116                     & Project ITS and VSC        \\ \hline
    \end{tabular}
    \caption{Comparison of datasets (Sort by Year)}
    \label{tab:datasets}
\end{table}
