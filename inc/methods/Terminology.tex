\subsection{Terminology}

Here is an updated list of key terminology and vocabulary that you may need to know before studying Text-to-SQL language models:

% \subsubsection{SQL (Structured Query Language)} The standard, a widely used programming language designed to manage relational databases, enables users to store, retrieve and manipulate data.
% \subsubsection{Natural Language Processing (NLP)}
% The field of study focuses on the interaction between human language and computers, which ranges from understanding spoken language to generating natural language text.

\subsubsection{Pre-training and Fine-tuning}

Pre-training refers to training a model on a large dataset and then fine-tuning it on a smaller dataset for a specific task, which helps to improve the model's performance on the specific task.

\subsubsection{SQL Constructs}

The elements of SQL language such as SELECT, FROM, WHERE, JOIN, are used to build queries and retrieve data from a database.

% \subsubsection{Evaluation Metrics}

% Measures used to evaluate the performance of Text-to-SQL models, such as accuracy, F1-score, and Exact Match score, compare different models and determine the best-performing model.

\subsubsection{Baseline Model}

A model that serves as a reference point or starting point for comparison, providing a baseline for performance against which other models can be evaluated.


% \subsubsection{Encoder-Decoder Architecture}

% A robust neural network architecture that utilizes an encoder\cite{cho-etal-2014-learning} to transform the input data into a compact and meaningful representation and a decoder to generate the desired output from that representation. This architecture has been widely utilized in many applications, such as language translation, image captioning, and text summarization, to produce high-quality results. Furthermore, the encoder-decoder architecture has the advantage of learning complex relationships between input and output, making it a suitable tool for many challenging tasks\cite{kumar2022deep}.


\subsubsection{Self-attention}

Self-attention \cite{https://doi.org/10.48550/arxiv.1706.03762} is a mechanism used in the transformer architecture that allows the model to determine the significance of various components of the input sequence to be able to generate an outcome that is more precise and sufficient. This mechanism allows the model to consider the relationships between different parts of the input sequence and to factor those relationships into its output. Further, self-attention lets the model capture patterns from the input sequence and use those patterns to generate more meaningful output. It is this combination of factors that makes self-attention such an essential tool for deep learning models.

\subsubsection{Incremental decoding}

A decoding strategy where the model generates a sequence of tokens one at a time, at each step conditioned on the previous tokens, the input, and the context of the sentence. This approach allows for a more dynamic and flexible generation of output, as it takes into account a variety of factors when making decisions about the next token. This strategy also helps the model avoid repeating itself, providing more diverse and unique outputs. Furthermore, incremental decoding helps the model to capture the nuance of the language better as it is able to build upon previous decisions and refine its output as it progresses\cite{huang-mi-2010-efficient}.

\subsubsection{Semantic parsing}

Semantic parsing\cite{krishnamurthy-etal-2017-neural} is an area of natural language processing that involves extracting the meaning or intent from text. One class of Semantic Parsing, Text-to-SQL, involves converting natural language problems into SQL query statements. This is a challenging task, one that requires the use of advanced machine learning and natural language processing algorithms. As such, the research conducted in this field seeks to explore the various solutions and practices employed by researchers to tackle this problem effectively. Furthermore, it is also important to note that this problem is not just limited to converting natural language into SQL query statements, as other applications of Semantic Parsing have been explored, such as \ac{NLG}. Overall, by understanding the various techniques used for Semantic Parsing, we can better understand the complexities involved in this task and how best to approach it.
