\subsection{Decoding}

Decoders \cite{cho-etal-2014-learning} form an integral part of sequence-to-sequence models in natural language processing tasks, and they are constructed as a multi-layered architecture of recurrent elements, such as Long Short-Term Memory (LSTM) units, Gated Recurrent Units (GRUs), or other analogous structures. The primary responsibility of a decoder is to generate an output sequence by predicting an output, denoted as y, for each time step. This output sequence can be a series of words, phrases, or even entire sentences, depending on the specific problem being addressed.

At each time step, the current recurrent unit within the decoder receives a hidden state from the preceding recurrent unit. This hidden state encapsulates the information gathered up to that point and serves as a vital input for the current recurrent unit to make an informed prediction. Moreover, decoders can also incorporate attention mechanisms to help focus on the most relevant parts of the input sequence when generating the output. This is particularly useful in tasks that require the decoder to selectively attend to different input elements during the decoding process.

Decoders are commonly employed in a wide range of natural language processing applications \cite{kumar2022deep}, including but not limited to, machine translation, text summarization, question-answering systems, and dialogue generation. In question-answering tasks, for instance, the output sequence generated by the decoder is often a collection of words.

Numerous approaches have been suggested to enhance the decoding process for more precise and efficient SQL generation, ultimately bridging the divide between natural language and SQL query formulation. As illustrated in the table below, we have classified these techniques into five primary categories, along with additional methodologies\cite{deng2022recent}.

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        \rowcolor{Gray}
        \textbf{Methods}                                           & \textbf{Adopted by} & \textbf{Applied datasets} & \textbf{Addressed challenges}                                                            \\
        \hline
        \multirow{3}{*}{Tree-based}                                & Seq2Tree            & -                         & \multirow{3}{*}{Hierarchical decoding}                                                   \\
                                                                   & Seq2AST             & -                         &                                                                                          \\
                                                                   & SyntaxSQLNet        & Spider                    &                                                                                          \\
        \hline
        \multirow{4}{*}{Sketch-based}                              & SQLNet              & WikiSQL                   & \multirow{4}{*}{Hierarchical decoding}                                                   \\
                                                                   & Coarse2Fine         & WikiSQL                   &                                                                                          \\
                                                                   & IRNet               & Spider                    &                                                                                          \\
                                                                   & RYANSQL             & Spider                    &                                                                                          \\
        \hline
        Bottom-up                                                  & SmBop               & Spider                    & Hierarchical decoding                                                                    \\
        \hline
        \multirow{2}{*}{Self-Attention}                            & Seq2Tree            & -                         & \multirow{2}{*}{ Synthesizing information}                                               \\
                                                                   & Seq2SQL             & WikiSQL                   &                                                                                          \\
        \hline
        Bi-attention                                               & BiSQL               & Spider                    & Synthesizing information                                                                 \\
        \hline
        \parbox{3cm}{Relation-aware Self-attention}                & DuoRAT              & Spider                    & Synthesizing information                                                                 \\
        \hline
        \multirow{3}{*}{Copy Mechanism}                            & Seq2AST             & -                         & \multirow{3}{*}{ Synthesizing information}                                               \\
                                                                   & Seq2SQL             & WikiSQL                   &                                                                                          \\
                                                                   & SeqGenSQL           & WikiSQL                   &                                                                                          \\
        \hline
        \multirow{3}{*}{\parbox{3cm}{Intermediate Representation}} & IncSQL              & WikiSQL                   & \multirow{3}{*}{{\parbox{5cm}{Bridging the gap between natural language and SQL query}}} \\
                                                                   & IRNet               & WikiSQL                   &                                                                                          \\
                                                                   & ValueNet            & Spider                    &                                                                                          \\
        \hline
        Constrained decoding                                       & PICARD              & Spider                    & Fine-grained decoding                                                                    \\
        % \hline
        % Execution-guided                                           & SQLova              & WikiSQL                   & Fine-grained decoding                                                                    \\
        % \hline
        % Separate submodule                                         & SQLNet              & WikiSQL                   & Easier decoding                                                                          \\
        % \hline
        % BPE                                                        & BPESQL              & Advising, ATIS
        %    & Easier decoding
        % \\
        \hline
    \end{tabular}
    \caption{Methods used for decoding in text-to-SQL \cite{deng2022recent}}
    \label{tab:decoders}
\end{table}

\input{inc/methods/decoders/Tree-based.tex}
\input{inc/methods/decoders/Sketch-based.tex}
\input{inc/methods/decoders/Bottom-up.tex}
\input{inc/methods/decoders/Attention-Mechanism.tex}
\input{inc/methods/decoders/Copy-Mechanism.tex}
\input{inc/methods/decoders/Intermediate-Representations.tex}
\input{inc/methods/decoders/Constrained.tex}

% Schema linking is a component of text-to-SQL models that helps map natural language phrases to elements of a database schema.
% Skeleton parsing is a component of text-to-SQL models that helps generate the structure of an SQL query based on a natural language question. It focuses on generating the pure skeleton of an SQL query (i.e., SQL keywords).