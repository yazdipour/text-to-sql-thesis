\subsection{Decoding}

Decoders \cite{cho-etal-2014-learning} form an integral part of sequence-to-sequence models in natural language processing tasks, and they are constructed as a multi-layered architecture of recurrent elements, such as Long Short-Term Memory (LSTM) units, Gated Recurrent Units (GRUs), or other analogous structures. The primary responsibility of a decoder is to generate an output sequence by predicting an output, denoted as y, for each time step. This output sequence can be a series of words, phrases, or even entire sentences, depending on the specific problem being addressed.

At each time step, the current recurrent unit within the decoder receives a hidden state from the preceding recurrent unit. This hidden state encapsulates the information gathered up to that point and serves as a vital input for the current recurrent unit to make an informed prediction. Moreover, decoders can also incorporate attention mechanisms to help focus on the most relevant parts of the input sequence when generating the output. This is particularly useful in tasks that require the decoder to selectively attend to different input elements during the decoding process.

Decoders are commonly employed in a wide range of natural language processing applications \cite{kumar2022deep}, including but not limited to, machine translation, text summarization, question-answering systems, and dialogue generation. In question-answering tasks, for instance, the output sequence generated by the decoder is often a collection of words.

Numerous approaches have been suggested to enhance the decoding process for more precise and efficient SQL generation, ultimately bridging the divide between natural language and SQL query formulation. As illustrated in the table below, we have classified these techniques into five primary categories, along with additional methodologies\cite{deng2022recent}.

\begin{table}[H]
    \centering
    \begin{tabular}{cccc}
        \hline
        \rowcolor{Gray}
        \textbf{Methods}                                           & \textbf{Adopted by} & \textbf{Applied datasets} & \textbf{Addressed challenges}                                                            \\
        \hline
        \multirow{3}{*}{Tree-based}                                & Seq2Tree            & -                         & \multirow{3}{*}{Hierarchical decoding}                                                   \\
                                                                   & Seq2AST             & -                         &                                                                                          \\
                                                                   & SyntaxSQLNet        & Spider                    &                                                                                          \\
        \hline
        \multirow{4}{*}{Sketch-based}                              & SQLNet              & WikiSQL                   & \multirow{4}{*}{Hierarchical decoding}                                                   \\
                                                                   & Coarse2Fine         & WikiSQL                   &                                                                                          \\
                                                                   & IRNet               & Spider                    &                                                                                          \\
                                                                   & RYANSQL             & Spider                    &                                                                                          \\
        \hline
        Bottom-up                                                  & SmBop               & Spider                    & Hierarchical decoding                                                                    \\
        \hline
        \multirow{2}{*}{Self-Attention}                            & Seq2Tree            & -                         & \multirow{2}{*}{ Synthesizing information}                                               \\
                                                                   & Seq2SQL             & WikiSQL                   &                                                                                          \\
        \hline
        Bi-attention                                               & BiSQL               & Spider                    & Synthesizing information                                                                 \\
        \hline
        \parbox{3cm}{Relation-aware Self-attention}                & DuoRAT              & Spider                    & Synthesizing information                                                                 \\
        \hline
        \multirow{3}{*}{Copy Mechanism}                            & Seq2AST             & -                         & \multirow{3}{*}{ Synthesizing information}                                               \\
                                                                   & Seq2SQL             & WikiSQL                   &                                                                                          \\
                                                                   & SeqGenSQL           & WikiSQL                   &                                                                                          \\
        \hline
        \multirow{3}{*}{\parbox{3cm}{Intermediate Representation}} & IncSQL              & WikiSQL                   & \multirow{3}{*}{{\parbox{5cm}{Bridging the gap between natural language and SQL query}}} \\
                                                                   & IRNet               & WikiSQL                   &                                                                                          \\
                                                                   & ValueNet            & Spider                    &                                                                                          \\
        \hline
        Constrained decoding                                       & PICARD              & Spider                    & Fine-grained decoding                                                                    \\
        % \hline
        % Execution-guided                                           & SQLova              & WikiSQL                   & Fine-grained decoding                                                                    \\
        % \hline
        % Separate submodule                                         & SQLNet              & WikiSQL                   & Easier decoding                                                                          \\
        % \hline
        % BPE                                                        & BPESQL              & Advising, ATIS
        %    & Easier decoding
        % \\
        \hline
    \end{tabular}
    \caption{Methods used for decoding in text-to-SQL \cite{deng2022recent}}
    \label{tab:decoders}
\end{table}

% \clearpage
\subsubsection{Argmax Decoding}

Argmax decoding is a straightforward method where the word with the highest probability in the predicted probability distribution is chosen at each time step. The process is repeated until a designated end-of-sentence token is generated or a maximum length is reached. This approach is computationally efficient but tends to produce suboptimal sequences due to its greedy nature.

\subsubsection{Sampling}

Sampling is a stochastic decoding technique that selects the next word based on the probability distribution generated by the model. This approach allows for more diversity in the generated text, as it can explore lower-probability words that may still be semantically relevant. However, sampling can sometimes lead to less coherent text, as it may generate rare or unrelated words due to the stochastic nature of the process. Top-K sampling is a variation of the sampling method, where only the top K most probable words are considered for sampling. This approach reduces the risk of generating irrelevant or rare words, while still allowing for diversity in the generated text.

\input{inc/methods/decoders/Tree-based.tex}
\input{inc/methods/decoders/Sketch-based.tex}
\input{inc/methods/decoders/Bottom-up.tex}
\input{inc/methods/decoders/Attention-Mechanism.tex}
\input{inc/methods/decoders/Copy-Mechanism.tex}
\input{inc/methods/decoders/Intermediate-Representations.tex}
\input{inc/methods/decoders/Constrained.tex}
\input{inc/methods/decoders/beam.tex}
\input{inc/models/Picard.tex}

% Schema linking is a component of text-to-SQL models that helps map natural language phrases to elements of a database schema.
% Skeleton parsing is a component of text-to-SQL models that helps generate the structure of an SQL query based on a natural language question. It focuses on generating the pure skeleton of an SQL query (i.e., SQL keywords).