\section{Conclusion}

In this thesis, we studied cross-view learning with limited supervision. We provided both empirical
and theoretical analyses that how we can leverage the information across different data views to learn
good representations when having access to only limited supervision signals (e.g., without supervised
labels or with only auxiliary information of data). This chapter provides a succinct summary of the main
contributions, and then we discuss certain limitations of our work. Studying these limitations helps us
better understand the topic - cross-view learning with limited supervision - and hence attempting to address
these limitations can be potential future research directions.

\input{inc/Future}

\subsection{Limitations of the study}

A first observation about this thesis is that we considered multi-view data from mostly two or three different
views (e.g., the human multi-modal utterance with visual, acoustic, and textual views). While this was
an important stepping stone, data can often include a larger number of views, such as signals for aircraft
sensors that track oil temperature, fuel pressure, air speed measurement, lightening detection, vibration
detection, etc. Some of our proposed multi-view representation learning approaches may have trouble to
scale up to larger number of views, and we acknowledge this potential limitation as representation learning
with a large number of views. Second, our empirical and theoretical analysis on self-supervised learning lie
mainly within visual modality. In particular, we considered augmented variants as different views of an
image, and then we analyzed why the self-supervised learned representations can reach good downstream
performance and the practical deployments of different self-supervised objectives. Although we manifested
good results in visual modality, we have not yet shown that our approaches can generalize to other modality,
such as audio or textual modalities. We identify this limitation as self-supervised learning beyond visual
modality. Lastly, when studying multi-view representation learning, the thesis focuses primarily on the task
of perception and less about action generation (e.g., action generation for navigation). The perception and
action generation procedures are two important phases for an intelligent agent, where the perception phase
receives multi-view signals and transfers them into high-level representations, and the action generation
phase takes the internal representations and generates actions. Our thesis does not directly tackle the
problem of action generation in multi-view representation learning, such as the movement of a robot after
receiving the multi-view sensory signals (e.g., visual inputs from camera or distance measurements from
ultrasonic sensors). We identify this limitation as multi-view representation learning for action generation.
In the following sub-section, we discuss these potential limitations and point towards promising future
research directions.

\subsection{Conclusion and Summary of findings}

In this chapter, we studied ...