@article{xu_sqlnet_2017,
	title = {{SQLNet}: Generating Structured Queries From Natural Language Without Reinforcement Learning},
	url = {http://arxiv.org/abs/1711.04436},
	shorttitle = {{SQLNet}},
	abstract = {Synthesizing {SQL} queries from natural language is a long-standing open problem and has been attracting considerable interest recently. Toward solving the problem, the de facto approach is to employ a sequence-to-sequence-style model. Such an approach will necessarily require the {SQL} queries to be serialized. Since the same {SQL} query may have multiple equivalent serializations, training a sequence-to-sequence-style model is sensitive to the choice from one of them. This phenomenon is documented as the "order-matters" problem. Existing state-of-the-art approaches rely on reinforcement learning to reward the decoder when it generates any of the equivalent serializations. However, we observe that the improvement from reinforcement learning is limited. In this paper, we propose a novel approach, i.e., {SQLNet}, to fundamentally solve this problem by avoiding the sequence-to-sequence structure when the order does not matter. In particular, we employ a sketch-based approach where the sketch contains a dependency graph so that one prediction can be done by taking into consideration only the previous predictions that it depends on. In addition, we propose a sequence-to-set model as well as the column attention mechanism to synthesize the query based on the sketch. By combining all these novel techniques, we show that {SQLNet} can outperform the prior art by 9\% to 13\% on the {WikiSQL} task.},
	journaltitle = {{arXiv}:1711.04436 [cs]},
	author = {Xu, Xiaojun and Liu, Chang and Song, Dawn},
	urldate = {2022-05-12},
	date = {2017-11-13},
	eprinttype = {arxiv},
	eprint = {1711.04436},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Databases},
}

@article{raffel_exploring_2020,
  author    = {Colin Raffel and
               Noam Shazeer and
               Adam Roberts and
               Katherine Lee and
               Sharan Narang and
               Michael Matena and
               Yanqi Zhou and
               Wei Li and
               Peter J. Liu},
  title     = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
               Transformer},
  journal   = {CoRR},
  volume    = {abs/1910.10683},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.10683},
  eprinttype = {arXiv},
  eprint    = {1910.10683},
  timestamp = {Fri, 05 Feb 2021 15:43:41 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-10683.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{cai_sadga_2022,
	title = {{SADGA}: Structure-Aware Dual Graph Aggregation Network for Text-to-{SQL}},
	url = {http://arxiv.org/abs/2111.00653},
	shorttitle = {{SADGA}},
	abstract = {The Text-to-{SQL} task, aiming to translate the natural language of the questions into {SQL} queries, has drawn much attention recently. One of the most challenging problems of Text-to-{SQL} is how to generalize the trained model to the unseen database schemas, also known as the cross-domain Text-to-{SQL} task. The key lies in the generalizability of (i) the encoding method to model the question and the database schema and (ii) the question-schema linking method to learn the mapping between words in the question and tables/columns in the database schema. Focusing on the above two key issues, we propose a Structure-Aware Dual Graph Aggregation Network ({SADGA}) for cross-domain Text-to-{SQL}. In {SADGA}, we adopt the graph structure to provide a unified encoding model for both the natural language question and database schema. Based on the proposed unified modeling, we further devise a structure-aware aggregation method to learn the mapping between the question-graph and schema-graph. The structure-aware aggregation method is featured with Global Graph Linking, Local Graph Linking, and Dual-Graph Aggregation Mechanism. We not only study the performance of our proposal empirically but also achieved 3rd place on the challenging Text-to-{SQL} benchmark Spider at the time of writing.},
	journaltitle = {arXiv:2111.00653 [cs]},
	author = {Cai, Ruichu and Yuan, Jinjie and Xu, Boyan and Hao, Zhifeng},
	urldate = {2022-05-07},
	date = {2022-01-17},
	eprinttype = {arxiv},
	eprint = {2111.00653},
	keywords = {Computer Science - Computation and Language},
	
}

@inproceedings{Scholak2021:PICARD,
  author = {Torsten Scholak and Nathan Schucher and Dzmitry Bahdanau},
  title = "{PICARD}: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
  month = nov,
  year = "2021",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.emnlp-main.779",
  pages = "9895--9901",
}

@article{shi_learning_2020,
	title = {Learning Contextual Representations for Semantic Parsing with Generation-Augmented Pre-Training},
	url = {http://arxiv.org/abs/2012.10309},
	abstract = {Most recently, there has been significant interest in learning contextual representations for various {NLP} tasks, by leveraging large scale text corpora to train large neural language models with self-supervised learning objectives, such as Masked Language Model ({MLM}). However, based on a pilot study, we observe three issues of existing general-purpose language models when they are applied to text-to-{SQL} semantic parsers: fail to detect column mentions in the utterances, fail to infer column mentions from cell values, and fail to compose complex {SQL} queries. To mitigate these issues, we present a model pre-training framework, Generation-Augmented Pre-training ({GAP}), that jointly learns representations of natural language utterances and table schemas by leveraging generation models to generate pre-train data. {GAP} {MODEL} is trained on 2M utterance-schema pairs and 30K utterance-schema-{SQL} triples, whose utterances are produced by generative models. Based on experimental results, neural semantic parsers that leverage {GAP} {MODEL} as a representation encoder obtain new state-of-the-art results on both {SPIDER} and {CRITERIA}-{TO}-{SQL} benchmarks.},
	journaltitle = {{arXiv}:2012.10309 [cs]},
	author = {Shi, Peng and Ng, Patrick and Wang, Zhiguo and Zhu, Henghui and Li, Alexander Hanbo and Wang, Jun and Santos, Cicero Nogueira dos and Xiang, Bing},
	urldate = {2022-05-07},
	date = {2020-12-18},
	eprinttype = {arxiv},
	eprint = {2012.10309},
	note = {version: 1},
	keywords = {Computer Science - Computation and Language},
	
}

@article{choi_ryansql_2020,
	title = {{RYANSQL}: Recursively Applying Sketch-based Slot Fillings for Complex Text-to-{SQL} in Cross-Domain Databases},
	url = {http://arxiv.org/abs/2004.03125},
	shorttitle = {{RYANSQL}},
	abstract = {Text-to-{SQL} is the problem of converting a user question into an {SQL} query, when the question and database are given. In this paper, we present a neural network approach called {RYANSQL} (Recursively Yielding Annotation Network for {SQL}) to solve complex Text-to-{SQL} tasks for cross-domain databases. State-ment Position Code ({SPC}) is defined to trans-form a nested {SQL} query into a set of non-nested {SELECT} statements; a sketch-based slot filling approach is proposed to synthesize each {SELECT} statement for its corresponding {SPC}. Additionally, two input manipulation methods are presented to improve generation performance further. {RYANSQL} achieved 58.2\% accuracy on the challenging Spider benchmark, which is a 3.2\%p improvement over previous state-of-the-art approaches. At the time of writing, {RYANSQL} achieves the first position on the Spider leaderboard.},
	journaltitle = {{arXiv}:2004.03125 [cs]},
	author = {Choi, {DongHyun} and Shin, Myeong Cheol and Kim, {EungGyun} and Shin, Dong Ryeol},
	urldate = {2022-05-07},
	date = {2020-04-07},
	eprinttype = {arxiv},
	eprint = {2004.03125},
	note = {version: 1},
	keywords = {Computer Science - Computation and Language}
}

@article{yin_tabert_2020,
	title = {{TaBERT}: Pretraining for Joint Understanding of Textual and Tabular Data},
	url = {http://arxiv.org/abs/2005.08314},
	shorttitle = {{TaBERT}},
	abstract = {Recent years have witnessed the burgeoning of pretrained language models ({LMs}) for text-based natural language ({NL}) understanding tasks. Such models are typically trained on free-form {NL} text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form {NL} questions and structured tabular data (e.g., database tables). In this paper we present {TaBERT}, a pretrained {LM} that jointly learns representations for {NL} sentences and (semi-)structured tables. {TaBERT} is trained on a large corpus of 26 million tables and their English contexts. In experiments, neural semantic parsers using {TaBERT} as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark {WikiTableQuestions}, while performing competitively on the text-to-{SQL} dataset Spider. Implementation of the model will be available at http://fburl.com/{TaBERT} .},
	journaltitle = {{arXiv}:2005.08314 [cs]},
	author = {Yin, Pengcheng and Neubig, Graham and Yih, Wen-tau and Riedel, Sebastian},
	urldate = {2022-05-07},
	date = {2020-05-17},
	eprinttype = {arxiv},
	eprint = {2005.08314},
	note = {version: 1},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning}
}

@article{yu_spider_2019,
	title = {Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-{SQL} Task},
	url = {http://arxiv.org/abs/1809.08887},
	shorttitle = {Spider},
	abstract = {We present Spider, a large-scale, complex and cross-domain semantic parsing and text-to-{SQL} dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex {SQL} queries on 200 databases with multiple tables, covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-{SQL} task where different complex {SQL} queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new {SQL} queries and new database schemas. Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and the exact same programs in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 12.4\% exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task are publicly available at https://yale-lily.github.io/spider},
	journaltitle = {{arXiv}:1809.08887 [cs]},
	author = {Yu, Tao and Zhang, Rui and Yang, Kai and Yasunaga, Michihiro and Wang, Dongxu and Li, Zifan and Ma, James and Li, Irene and Yao, Qingning and Roman, Shanelle and Zhang, Zilin and Radev, Dragomir},
	urldate = {2022-05-07},
	date = {2019-02-02},
	eprinttype = {arxiv},
  year = {2018},
	eprint = {1809.08887},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	
}

@article{zhong_semantic_2020,
	title = {Semantic Evaluation for Text-to-{SQL} with Distilled Test Suites},
	url = {http://arxiv.org/abs/2010.02840},
	abstract = {We propose test suite accuracy to approximate semantic accuracy for Text-to-{SQL} models. Our method distills a small test suite of databases that achieves high code coverage for the gold query from a large number of randomly generated databases. At evaluation time, it computes the denotation accuracy of the predicted queries on the distilled test suite, hence calculating a tight upper-bound for semantic accuracy efficiently. We use our proposed method to evaluate 21 models submitted to the Spider leader board and manually verify that our method is always correct on 100 examples. In contrast, the current Spider metric leads to a 2.5\% false negative rate on average and 8.1\% in the worst case, indicating that test suite accuracy is needed. Our implementation, along with distilled test suites for eleven Text-to-{SQL} datasets, is publicly available.},
	journaltitle = {{arXiv}:2010.02840 [cs]},
	author = {Zhong, Ruiqi and Yu, Tao and Klein, Dan},
	urldate = {2022-05-08},
	date = {2020-10-06},
	eprinttype = {arxiv},
	eprint = {2010.02840},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
	
}

@article{wang_text--sql_2020,
	title = {Text-to-{SQL} Generation for Question Answering on Electronic Medical Records},
	url = {http://arxiv.org/abs/1908.01839},
	abstract = {Electronic medical records ({EMR}) contain comprehensive patient information and are typically stored in a relational database with multiple tables. Effective and efficient patient information retrieval from {EMR} data is a challenging task for medical experts. Question-to-{SQL} generation methods tackle this problem by first predicting the {SQL} query for a given question about a database, and then, executing the query on the database. However, most of the existing approaches have not been adapted to the healthcare domain due to a lack of healthcare Question-to-{SQL} dataset for learning models specific to this domain. In addition, wide use of the abbreviation of terminologies and possible typos in questions introduce additional challenges for accurately generating the corresponding {SQL} queries. In this paper, we tackle these challenges by developing a deep learning based {TRanslate}-Edit Model for Question-to-{SQL} ({TREQS}) generation, which adapts the widely used sequence-to-sequence model to directly generate the {SQL} query for a given question, and further performs the required edits using an attentive-copying mechanism and task-specific look-up tables. Based on the widely used publicly available electronic medical database, we create a new large-scale Question-{SQL} pair dataset, named {MIMICSQL}, in order to perform the Question-to-{SQL} generation task in healthcare domain. An extensive set of experiments are conducted to evaluate the performance of our proposed model on {MIMICSQL}. Both quantitative and qualitative experimental results indicate the flexibility and efficiency of our proposed method in predicting condition values and its robustness to random questions with abbreviations and typos.},
	journaltitle = {{arXiv}:1908.01839 [cs]},
	author = {Wang, Ping and Shi, Tian and Reddy, Chandan K.},
	urldate = {2022-05-09},
	date = {2020-01-29},
	eprinttype = {arxiv},
	eprint = {1908.01839},
	note = {version: 2},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Information Retrieval},
	
}

@article{zhong_seq2sql_2017,
	title = {Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning},
	url = {http://arxiv.org/abs/1709.00103},
	shorttitle = {Seq2SQL},
	abstract = {A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as {SQL}. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding {SQL} queries. Our model leverages the structure of {SQL} queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish {WikiSQL}, a dataset of 80654 hand-annotated examples of questions and {SQL} queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to {WikiSQL}, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9\% to 59.4\% and logical form accuracy from 23.4\% to 48.3\%.},
	journaltitle = {{arXiv}:1709.00103 [cs]},
	author = {Zhong, Victor and Xiong, Caiming and Socher, Richard},
	urldate = {2022-05-12},
	date = {2017-11-09},
	eprinttype = {arxiv},
	eprint = {1709.00103},
	note = {version: 6},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	
}

@article{lin_bridging_2020,
	title = {Bridging Textual and Tabular Data for Cross-Domain Text-to-{SQL} Semantic Parsing},
	url = {http://arxiv.org/abs/2012.12627},
	abstract = {We present {BRIDGE}, a powerful sequential architecture for modeling dependencies between natural language questions and relational databases in cross-{DB} semantic parsing. {BRIDGE} represents the question and {DB} schema in a tagged sequence where a subset of the fields are augmented with cell values mentioned in the question. The hybrid sequence is encoded by {BERT} with minimal subsequent layers and the text-{DB} contextualization is realized via the fine-tuned deep attention in {BERT}. Combined with a pointer-generator decoder with schema-consistency driven search space pruning, {BRIDGE} attained state-of-the-art performance on popular cross-{DB} text-to-{SQL} benchmarks, Spider (71.1{\textbackslash}\% dev, 67.5{\textbackslash}\% test with ensemble model) and {WikiSQL} (92.6{\textbackslash}\% dev, 91.9{\textbackslash}\% test). Our analysis shows that {BRIDGE} effectively captures the desired cross-modal dependencies and has the potential to generalize to more text-{DB} related tasks. Our implementation is available at {\textbackslash}url\{https://github.com/salesforce/{TabularSemanticParsing}\}.},
	journaltitle = {{arXiv}:2012.12627 [cs]},
	author = {Lin, Xi Victoria and Socher, Richard and Xiong, Caiming},
	urldate = {2022-05-12},
	date = {2020-12-30},
	eprinttype = {arxiv},
	eprint = {2012.12627},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Databases, Computer Science - Machine Learning},
	
}

@article{wang_rat-sql_2021,
	title = {{RAT}-{SQL}: Relation-Aware Schema Encoding and Linking for Text-to-{SQL} Parsers},
	url = {http://arxiv.org/abs/1911.04942},
	shorttitle = {{RAT}-{SQL}},
	abstract = {When translating natural language questions into {SQL} queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas. The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query. We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-{SQL} encoder. On the challenging Spider dataset this framework boosts the exact match accuracy to 57.2\%, surpassing its best counterparts by 8.7\% absolute improvement. Further augmented with {BERT}, it achieves the new state-of-the-art performance of 65.6\% on the Spider leaderboard. In addition, we observe qualitative improvements in the model's understanding of schema linking and alignment. Our implementation will be open-sourced at https://github.com/Microsoft/rat-sql.},
	journaltitle = {{arXiv}:1911.04942 [cs]},
	author = {Wang, Bailin and Shin, Richard and Liu, Xiaodong and Polozov, Oleksandr and Richardson, Matthew},
	urldate = {2022-05-12},
	date = {2021-08-24},
	eprinttype = {arxiv},
	eprint = {1911.04942},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	
}

@article{lyu_hybrid_2020,
	title = {Hybrid Ranking Network for Text-to-{SQL}},
	url = {http://arxiv.org/abs/2008.04759},
	abstract = {In this paper, we study how to leverage pre-trained language models in Text-to-{SQL}. We argue that previous approaches under utilize the base language models by concatenating all columns together with the {NL} question and feeding them into the base language model in the encoding stage. We propose a neat approach called Hybrid Ranking Network ({HydraNet}) which breaks down the problem into column-wise ranking and decoding and finally assembles the column-wise outputs into a {SQL} query by straightforward rules. In this approach, the encoder is given a {NL} question and one individual column, which perfectly aligns with the original tasks {BERT}/{RoBERTa} is trained on, and hence we avoid any ad-hoc pooling or additional encoding layers which are necessary in prior approaches. Experiments on the {WikiSQL} dataset show that the proposed approach is very effective, achieving the top place on the leaderboard.},
	journaltitle = {{arXiv}:2008.04759 [cs]},
	author = {Lyu, Qin and Chakrabarti, Kaushik and Hathi, Shobhit and Kundu, Souvik and Zhang, Jianwen and Chen, Zheng},
	urldate = {2022-05-12},
	date = {2020-08-11},
	year = {2020},
	eprinttype = {arxiv},
	eprint = {2008.04759},
	keywords = {Computer Science - Computation and Language},	
}

@article{2020t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@article{DBLP:journals/corr/abs-1810-04805,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  eprinttype = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{DBLP:journals/corr/abs-2005-08314,
  author    = {Pengcheng Yin and
               Graham Neubig and
               Wen{-}tau Yih and
               Sebastian Riedel},
  title     = {TaBERT: Pretraining for Joint Understanding of Textual and Tabular
               Data},
  journal   = {CoRR},
  volume    = {abs/2005.08314},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.08314},
  eprinttype = {arXiv},
  eprint    = {2005.08314},
  timestamp = {Tue, 27 Jul 2021 11:52:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-08314.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{vig_comparison_2019,
	booktitle = {Comparison of Transfer-Learning Approaches for Response Selection in Multi-Turn Conversations},
	abstract = {This paper compares three transfer-learning approaches to response selection in dialogs, as part of the Dialog System Technology Challenge 7 (DSTC7) Track 1. In the first approach , Multi-Turn ESIM+ELMo (MT-EE), we incorporate pre-trained contextual embeddings into a sentence-pair model that was originally designed for natural language inference. In the second approach, we fine-tune the Generative Pre-trained Transformer (OpenAI GPT) model. In the third approach, we fine-tune the Bidirectional Encoder Representations from Transformers (BERT) model. Our results show that BERT performed best, followed by the GPT model and then the MT-EE model. We also discuss the relative advantages and disadvantages of each approach. The submitted result for Track 1 (MT-EE) placed second and fifth overall for the Advising and Ubuntu datasets respectively.},
	author = {Vig, Jesse and Ramea, Kalai},
	year = {2019}
}

@InProceedings{maas-EtAl:2011:ACL-HLT2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}

@article{DBLP:journals/corr/abs-2009-13845,
  author    = {Tao Yu and
               Chien{-}Sheng Wu and
               Xi Victoria Lin and
               Bailin Wang and
               Yi Chern Tan and
               Xinyi Yang and
               Dragomir R. Radev and
               Richard Socher and
               Caiming Xiong},
  title     = {GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing},
  journal   = {CoRR},
  volume    = {abs/2009.13845},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.13845},
  eprinttype = {arXiv},
  eprint    = {2009.13845},
  timestamp = {Wed, 12 May 2021 16:44:19 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-13845.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1810-05237,
  author    = {Tao Yu and
               Michihiro Yasunaga and
               Kai Yang and
               Rui Zhang and
               Dongxu Wang and
               Zifan Li and
               Dragomir R. Radev},
  title     = {SyntaxSQLNet: Syntax Tree Networks for Complex and Cross-DomainText-to-SQL
               Task},
  journal   = {CoRR},
  volume    = {abs/1810.05237},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.05237},
  eprinttype = {arXiv},
  eprint    = {1810.05237},
  timestamp = {Wed, 12 May 2021 16:44:19 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-05237.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/Rong14,
  author    = {Xin Rong},
  title     = {word2vec Parameter Learning Explained},
  journal   = {CoRR},
  volume    = {abs/1411.2738},
  year      = {2014},
  url       = {http://arxiv.org/abs/1411.2738},
  eprinttype = {arXiv},
  eprint    = {1411.2738},
  timestamp = {Mon, 13 Aug 2018 16:45:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Rong14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1909-00786,
  author    = {Rui Zhang and
               Tao Yu and
               Heyang Er and
               Sungrok Shim and
               Eric Xue and
               Xi Victoria Lin and
               Tianze Shi and
               Caiming Xiong and
               Richard Socher and
               Dragomir R. Radev},
  title     = {Editing-Based {SQL} Query Generation for Cross-Domain Context-Dependent
               Questions},
  journal   = {CoRR},
  volume    = {abs/1909.00786},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.00786},
  eprinttype = {arXiv},
  eprint    = {1909.00786},
  timestamp = {Wed, 12 May 2021 16:44:19 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-00786.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1905-08205,
  author    = {Jiaqi Guo and
               Zecheng Zhan and
               Yan Gao and
               Yan Xiao and
               Jian{-}Guang Lou and
               Ting Liu and
               Dongmei Zhang},
  title     = {Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate
               Representation},
  journal   = {CoRR},
  volume    = {abs/1905.08205},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.08205},
  eprinttype = {arXiv},
  eprint    = {1905.08205},
  timestamp = {Wed, 22 Jun 2022 11:17:20 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-08205.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  eprinttype = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/WuSCLNMKCGMKSJL16,
  author    = {Yonghui Wu and
               Mike Schuster and
               Zhifeng Chen and
               Quoc V. Le and
               Mohammad Norouzi and
               Wolfgang Macherey and
               Maxim Krikun and
               Yuan Cao and
               Qin Gao and
               Klaus Macherey and
               Jeff Klingner and
               Apurva Shah and
               Melvin Johnson and
               Xiaobing Liu and
               Lukasz Kaiser and
               Stephan Gouws and
               Yoshikiyo Kato and
               Taku Kudo and
               Hideto Kazawa and
               Keith Stevens and
               George Kurian and
               Nishant Patil and
               Wei Wang and
               Cliff Young and
               Jason Smith and
               Jason Riesa and
               Alex Rudnick and
               Oriol Vinyals and
               Greg Corrado and
               Macduff Hughes and
               Jeffrey Dean},
  title     = {Google's Neural Machine Translation System: Bridging the Gap between
               Human and Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1609.08144},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.08144},
  eprinttype = {arXiv},
  eprint    = {1609.08144},
  timestamp = {Thu, 14 Jan 2021 12:12:19 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/WuSCLNMKCGMKSJL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1901-11504,
  author    = {Xiaodong Liu and
               Pengcheng He and
               Weizhu Chen and
               Jianfeng Gao},
  title     = {Multi-Task Deep Neural Networks for Natural Language Understanding},
  journal   = {CoRR},
  volume    = {abs/1901.11504},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.11504},
  eprinttype = {arXiv},
  eprint    = {1901.11504},
  timestamp = {Mon, 30 May 2022 13:48:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-11504.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1902-01069,
  author    = {Wonseok Hwang and
               Jinyeung Yim and
               Seunghyun Park and
               Minjoon Seo},
  title     = {A Comprehensive Exploration on WikiSQL with Table-Aware Word Contextualization},
  journal   = {CoRR},
  volume    = {abs/1902.01069},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.01069},
  eprinttype = {arXiv},
  eprint    = {1902.01069},
  timestamp = {Tue, 21 May 2019 18:03:39 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-01069.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{10.1371/journal.pone.0211558,
    doi = {10.1371/journal.pone.0211558},
    author = {Moore, P. J. AND Lyons, T. J. AND Gallacher, J. AND for the Alzheimer’s Disease Neuroimaging Initiative},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Random forest prediction of Alzheimer’s disease using pairwise selection from time series data},
    year = {2019},
    month = {02},
    volume = {14},
    url = {https://doi.org/10.1371/journal.pone.0211558},
    pages = {1-14},
    abstract = {Time-dependent data collected in studies of Alzheimer’s disease usually has missing and irregularly sampled data points. For this reason time series methods which assume regular sampling cannot be applied directly to the data without a pre-processing step. In this paper we use a random forest to learn the relationship between pairs of data points at different time separations. The input vector is a summary of the time series history and it includes both demographic and non-time varying variables such as genetic data. To test the method we use data from the TADPOLE grand challenge, an initiative which aims to predict the evolution of subjects at risk of Alzheimer’s disease using demographic, physical and cognitive input data. The task is to predict diagnosis, ADAS-13 score and normalised ventricles volume. While the competition proceeds, forecasting methods may be compared using a leaderboard dataset selected from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) and with standard metrics for measuring accuracy. For diagnosis, we find an mAUC of 0.82, and a classification accuracy of 0.73 compared with a benchmark SVM predictor which gives mAUC = 0.62 and BCA = 0.52. The results show that the method is effective and comparable with other methods.},
    number = {2}
}

@inproceedings{roy2013the,
author = {Roy, Senjuti Basu and Cock, Martine De and Mandava, Vani and Savanna, Swapna and Dalessandro, Brian and Perlich, Claudia and Cukierski, William and Hamner, Ben},
title = {The Microsoft Academic Search Dataset and KDD Cup 2013 - Workshop for KDD Cup 2013},
year = {2013},
month = {August},
abstract = {KDD Cup 2013 challenged participants to tackle the problem of author name ambiguity in a digital library of scientific publications. The competition consisted of two tracks, which were based on large-scale datasets from a snapshot of Microsoft Academic Search, taken in January 2013 and including 250K authors and 2.5M papers. Participants were asked to determine which papers in an author profile are truly written by a given author (track 1), as well as to identify duplicate author profiles (track 2). Track 1 and track 2 were launched respectively on April 18 and April 20, 2013, with a common final submission deadline on June 12, 2013. For track 1 a training dataset with correct labels was diclosed at the start of the competition. This track was the most popular one, attracting submissions of 561 different teams. Track 2, which was formulated as an unsupervised learning task, received submissions from 241 participants. This paper presents details about the problem definitions, the datasets, the evaluation metrics and the results.},
publisher = {ACM - Association for Computing Machinery},
url = {https://www.microsoft.com/en-us/research/publication/the-microsoft-academic-search-dataset-and-kdd-cup-2013-workshop-for-kdd-cup-2013/},
}

@article{10.1145/3133887,
author = {Yaghmazadeh, Navid and Wang, Yuepeng and Dillig, Isil and Dillig, Thomas},
title = {SQLizer: Query Synthesis from Natural Language},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {OOPSLA},
url = {https://doi.org/10.1145/3133887},
doi = {10.1145/3133887},
abstract = {This paper presents a new technique for automatically synthesizing SQL queries from natural language (NL). At the core of our technique is a new NL-based program synthesis methodology that combines semantic parsing techniques from the NLP community with type-directed program synthesis and automated program repair. Starting with a program sketch obtained using standard parsing techniques, our approach involves an iterative refinement loop that alternates between probabilistic type inhabitation and automated sketch repair. We use the proposed idea to build an end-to-end system called SQLIZER that can synthesize SQL queries from natural language. Our method is fully automated, works for any database without requiring additional customization, and does not require users to know the underlying database schema. We evaluate our approach on over 450 natural language queries concerning three different databases, namely MAS, IMDB, and YELP. Our experiments show that the desired query is ranked within the top 5 candidates in close to 90\% of the cases and that SQLIZER outperforms NALIR, a state-of-the-art tool that won a best paper award at VLDB'14.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {63},
numpages = {26},
keywords = {Program Synthesis, Relational Databases, Programming by Natural Languages}
}

@InProceedings{10.1007/3-540-44795-4_40,
author="Tang, Lappoon R.
and Mooney, Raymond J.",
editor="De Raedt, Luc
and Flach, Peter",
title="Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing",
booktitle="Machine Learning: ECML 2001",
year="2001",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="466--477",
abstract="In this paper, we explored a learning approach which combines different learning methods in inductive logic programming (ILP) to allow a learner to produce more expressive hypotheses than that of each individual learner. Such a learning approach may be useful when the performance of the task depends on solving a large amount of classification problems and each has its own characteristics which may or may not fit a particular learning method. The task of semantic parser acquisition in two different domains was attempted and preliminary results demonstrated that such an approach is promising.",
isbn="978-3-540-44795-5"
}

@inproceedings{dahl-etal-1994-expanding,
    title = "Expanding the Scope of the {ATIS} Task: The {ATIS}-3 Corpus",
    author = "Dahl, Deborah A.  and
      Bates, Madeleine  and
      Brown, Michael  and
      Fisher, William  and
      Hunicke-Smith, Kate  and
      Pallett, David  and
      Pao, Christine  and
      Rudnicky, Alexander  and
      Shriberg, Elizabeth",
    booktitle = "{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",
    year = "1994",
    url = "https://aclanthology.org/H94-1010",
}

@inproceedings{cho-etal-2014-learning,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
}
@inproceedings{krishnamurthy-etal-2017-neural,
    title = "Neural Semantic Parsing with Type Constraints for Semi-Structured Tables",
    author = "Krishnamurthy, Jayant  and
      Dasigi, Pradeep  and
      Gardner, Matt",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1160",
    doi = "10.18653/v1/D17-1160",
    pages = "1516--1526",
    abstract = "We present a new semantic parsing model for answering compositional questions on semi-structured Wikipedia tables. Our parser is an encoder-decoder neural network with two key technical innovations: (1) a grammar for the decoder that only generates well-typed logical forms; and (2) an entity embedding and linking module that identifies entity mentions while generalizing across tables. We also introduce a novel method for training our neural model with question-answer supervision. On the WikiTableQuestions data set, our parser achieves a state-of-the-art accuracy of 43.3{\%} for a single model and 45.9{\%} for a 5-model ensemble, improving on the best prior score of 38.7{\%} set by a 15-model ensemble. These results suggest that type constraints and entity linking are valuable components to incorporate in neural semantic parsers.",
}

@inproceedings{finegan-dollak-etal-2018-improving,
    title = "Improving Text-to-{SQL} Evaluation Methodology",
    author = "Finegan-Dollak, Catherine  and
      Kummerfeld, Jonathan K.  and
      Zhang, Li  and
      Ramanathan, Karthik  and
      Sadasivam, Sesh  and
      Zhang, Rui  and
      Radev, Dragomir",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1033",
    doi = "10.18653/v1/P18-1033",
    pages = "351--360",
    abstract = "To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",
}

@inproceedings{huang-mi-2010-efficient,
    title = "Efficient Incremental Decoding for Tree-to-String Translation",
    author = "Huang, Liang  and
      Mi, Haitao",
    booktitle = "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2010",
    address = "Cambridge, MA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D10-1027",
    pages = "273--283",
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{Hochreiter1997LongSM,
  title={Long Short-Term Memory},
  author={Sepp Hochreiter and J{\"u}rgen Schmidhuber},
  journal={Neural Computation},
  year={1997},
  volume={9},
  pages={1735-1780}
}

@misc{https://doi.org/10.48550/arxiv.1409.3215,
  doi = {10.48550/ARXIV.1409.3215},
  
  url = {https://arxiv.org/abs/1409.3215},
  
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Sequence to Sequence Learning with Neural Networks},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.1706.03762,
  doi = {10.48550/ARXIV.1706.03762},
  
  url = {https://arxiv.org/abs/1706.03762},
  
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Attention Is All You Need},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{DBLP:journals/corr/abs-2106-05006,
author    = {Moshe Hazoom and
        Vibhor Malik and
        Ben Bogin},
title     = {Text-to-SQL in the Wild: {A} Naturally-Occurring Dataset Based on
Stack Exchange Data},
journal   = {CoRR},
volume    = {abs/2106.05006},
year      = {2021},
url       = {https://arxiv.org/abs/2106.05006},
eprinttype = {arXiv},
eprint    = {2106.05006},
timestamp = {Tue, 15 Jun 2021 16:35:15 +0200},
biburl    = {https://dblp.org/rec/journals/corr/abs-2106-05006.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{TOMOVA2022108211,
title = {SEOSS-Queries - a software engineering dataset for text-to-SQL and question answering tasks},
journal = {Data in Brief},
volume = {42},
pages = {108211},
year = {2022},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2022.108211},
url = {https://www.sciencedirect.com/science/article/pii/S2352340922004152},
author = {Mihaela Todorova Tomova and Martin Hofmann and Patrick Mäder},
keywords = {Software and systems requirement engineering, Text-to-SQL, Dataset, Question answering, Natural language processing},
abstract = {Stakeholders of software development projects have various information needs for making rational decisions during their daily work. Satisfying these needs requires substantial knowledge of where and how the relevant information is stored and consumes valuable time that is often not available. Easing the need for this knowledge is an ideal text-to-SQL benchmark problem, a field where public datasets are scarce and needed. We propose the SEOSS-Queries dataset consisting of natural language utterances and accompanying SQL queries extracted from previous studies, software projects, issue tracking tools, and through expert surveys to cover a large variety of information need perspectives. Our dataset consists of 1,162 English utterances translating into 166 SQL queries; each query has four precise utterances and three more general ones. Furthermore, the dataset contains 393,086 labeled utterances extracted from issue tracker comments. We provide pre-trained SQLNet and RatSQL baseline models for benchmark comparisons, a replication package facilitating a seamless application, and discuss various other tasks that may be solved and evaluated using the dataset. The whole dataset with paraphrased natural language utterances and SQL queries is hosted at figshare.com/s/75ed49ef01ac2f83b3e2.}
}

@article{RATH2019104005,
title = {The SEOSS 33 dataset — Requirements, bug reports, code history, and trace links for entire projects},
journal = {Data in Brief},
volume = {25},
pages = {104005},
year = {2019},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2019.104005},
url = {https://www.sciencedirect.com/science/article/pii/S2352340919303580},
author = {Michael Rath and Patrick Mäder},
keywords = {Mining software repositories, Data collection, Data mining, Requirements analysis, Traceability, Bug localization, Issue tracking, Changeset data},
abstract = {This paper provides a systematically retrieved dataset consisting of 33 open-source software projects containing a large number of typed artifacts and trace links between them. The artifacts stem from the projects' issue tracking system and source version control system to enable their joint analysis. Enriched with additional metadata, such as time stamps, release versions, component information, and developer comments, the dataset is highly suitable for empirical research, e.g., in requirements and software traceability analysis, software evolution, bug and feature localization, and stakeholder collaboration. It can stimulate new research directions, facilitate the replication of existing studies, and act as benchmark for the comparison of competing approaches. The data is hosted on Harvard Dataverse using DOI 10.7910/DVN/PDDZ4Q accessible via https://bit.ly/2wukCHc.}
}

@INPROCEEDINGS{8049176,
  author={Rath, Michael and Rempel, Patrick and Mäder, Patrick},
  booktitle={2017 IEEE 25th International Requirements Engineering Conference (RE)}, 
  title={The IlmSeven Dataset}, 
  year={2017},
  volume={},
  number={},
  pages={516-519},
  doi={10.1109/RE.2017.18}
}

@article{DBLP:journals/corr/abs-1804-09769,
  author    = {Tao Yu and
               Zifan Li and
               Zilin Zhang and
               Rui Zhang and
               Dragomir R. Radev},
  title     = {TypeSQL: Knowledge-based Type-Aware Neural Text-to-SQL Generation},
  journal   = {CoRR},
  volume    = {abs/1804.09769},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.09769},
  eprinttype = {arXiv},
  eprint    = {1804.09769},
  timestamp = {Sun, 02 Oct 2022 15:31:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-09769.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2103-04399,
  author    = {Binyuan Hui and
               Xiang Shi and
               Ruiying Geng and
               Binhua Li and
               Yongbin Li and
               Jian Sun and
               Xiaodan Zhu},
  title     = {Improving Text-to-SQL with Schema Dependency Learning},
  journal   = {CoRR},
  volume    = {abs/2103.04399},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.04399},
  eprinttype = {arXiv},
  eprint    = {2103.04399},
  timestamp = {Mon, 15 Mar 2021 17:30:55 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-04399.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{https://doi.org/10.48550/arxiv.2205.06983,
  doi = {10.48550/ARXIV.2205.06983},
  
  url = {https://arxiv.org/abs/2205.06983},
  
  author = {Qi, Jiexing and Tang, Jingyao and He, Ziwei and Wan, Xiangpeng and Cheng, Yu and Zhou, Chenghu and Wang, Xinbing and Zhang, Quanshi and Lin, Zhouhan},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Databases (cs.DB), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@inproceedings{Zelle1996LearningTP,
  title={Learning to Parse Database Queries Using Inductive Logic Programming},
  author={John M. Zelle and Raymond J. Mooney},
  booktitle={AAAI/IAAI, Vol. 2},
  year={1996}
}

@article{Warren1982AnEE,
  title={An Efficient Easily Adaptable System for Interpreting Natural Language Queries},
  author={David H. D. Warren and Fernando C Pereira},
  journal={Am. J. Comput. Linguistics},
  year={1982},
  volume={8},
  pages={110-122}
}

@article{DBLP:journals/corr/SutskeverVL14,
  author    = {Ilya Sutskever and
               Oriol Vinyals and
               Quoc V. Le},
  title     = {Sequence to Sequence Learning with Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1409.3215},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.3215},
  eprinttype = {arXiv},
  eprint    = {1409.3215},
  timestamp = {Mon, 13 Aug 2018 16:48:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SutskeverVL14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


% ULMFiT - ELMo
@article{ELMo,
  author    = {Jeremy Howard and
               Sebastian Ruder},
  title     = {Fine-tuned Language Models for Text Classification},
  journal   = {CoRR},
  volume    = {abs/1801.06146},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.06146},
  eprinttype = {arXiv},
  eprint    = {1801.06146},
  timestamp = {Mon, 13 Aug 2018 16:46:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1801-06146.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% GPT
@inproceedings{Radford2018ImprovingLU,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Alec Radford and Karthik Narasimhan},
  year={2018}
}

% 2017 openai
@article{DBLP:journals/corr/RadfordJS17,
  author    = {Alec Radford and
               Rafal J{\'{o}}zefowicz and
               Ilya Sutskever},
  title     = {Learning to Generate Reviews and Discovering Sentiment},
  journal   = {CoRR},
  volume    = {abs/1704.01444},
  year      = {2017},
  url       = {http://arxiv.org/abs/1704.01444},
  archivePrefix = {arXiv},
  eprint    = {1704.01444},
  timestamp = {Mon, 13 Aug 2018 16:47:25 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/RadfordJS17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{semql,
title = "SemQL: A semantic query language for multidatabase systems",
abstract = "An essential prerequisite to achieving interoperability in multidatabase systems is to be able to identify semantically equivalent or related data items in component databases. Another problem in multidatabase systems is allowing users to handle information from different databases that refer to the same real-world entity. In this paper, we provide semantic networks so that multidatabase systems can detect and resolve semantic heterogeneities among component databases. And we provide a semantic query language, SemQL, to capture the concepts about what users want. It enables users to issue queries to a large number of autonomous databases without prior knowledge of their schemas.",
author = "Lee, {Jeong Oog} and Baik, {Doo Kwon}",
year = "1999",
language = "English",
isbn = "1581131461",
series = "International Conference on Information and Knowledge Management, Proceedings",
publisher = "ACM",
pages = "259--266",
booktitle = "International Conference on Information and Knowledge Management, Proceedings",
note = "Proceedings of the 1999 8th International Conference on Information Knowledge Management (CIKM'99) ; Conference date: 02-11-1999 Through 06-11-1999",
}

@misc{2018MDM,
	note = {[Online; accessed 2023-01-29]},
	year = {2018},
	month = {1},
	title = {MDM and {Graph}},
	howpublished = {https://blog.semarchy.com/mdm-and-graph-edw-2018},
}

@inproceedings{shaw-etal-2021-compositional,
    title = "Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?",
    author = "Shaw, Peter  and
      Chang, Ming-Wei  and
      Pasupat, Panupong  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.75",
    doi = "10.18653/v1/2021.acl-long.75",
    pages = "922--938",
    abstract = "Sequence-to-sequence models excel at handling natural language variation, but have been shown to struggle with out-of-distribution compositional generalization. This has motivated new specialized architectures with stronger compositional biases, but most of these approaches have only been evaluated on synthetically-generated datasets, which are not representative of natural language variation. In this work we ask: can we develop a semantic parsing approach that handles both natural language variation and compositional generalization? To better assess this capability, we propose new train and test splits of non-synthetic datasets. We demonstrate that strong existing approaches do not perform well across a broad set of evaluations. We also propose NQG-T5, a hybrid model that combines a high-precision grammar-based approach with a pre-trained sequence-to-sequence model. It outperforms existing approaches across several compositional generalization challenges on non-synthetic data, while also being competitive with the state-of-the-art on standard evaluations. While still far from solving this problem, our study highlights the importance of diverse evaluations and the open challenge of handling both compositional generalization and natural language variation in semantic parsing.",
}
@article{10.1162/coli_a_00403,
    author = {Choi, DongHyun and Shin, Myeong Cheol and Kim, EungGyun and Shin, Dong Ryeol},
    title = "{RYANSQL: Recursively Applying Sketch-based Slot Fillings for Complex Text-to-SQL in Cross-Domain Databases}",
    journal = {Computational Linguistics},
    volume = {47},
    number = {2},
    pages = {309-332},
    year = {2021},
    month = {07},
    abstract = "{Text-to-SQL is the problem of converting a user question into an SQL query, when the question and database are given. In this article, we present a neural network approach called RYANSQL (Recursively Yielding Annotation Network for SQL) to solve complex Text-to-SQL tasks for cross-domain databases. Statement Position Code (SPC) is defined to transform a nested SQL query into a set of non-nested SELECT statements; a sketch-based slot-filling approach is proposed to synthesize each SELECT statement for its corresponding SPC. Additionally, two input manipulation methods are presented to improve generation performance further. RYANSQL achieved competitive result of 58.2\\% accuracy on the challenging Spider benchmark. At the time of submission (April 2020), RYANSQL v2, a variant of original RYANSQL, is positioned at 3rd place among all systems and 1st place among the systems not using database content with 60.6\\% exact matching accuracy. The source code is available at https://github.com/kakaoenterprise/RYANSQL.}",
    issn = {0891-2017},
    doi = {10.1162/coli_a_00403},
    url = {https://doi.org/10.1162/coli\_a\_00403},
    eprint = {https://direct.mit.edu/coli/article-pdf/47/2/309/1930983/coli\_a\_00403.pdf},
}


@inproceedings{min-etal-2019-pilot,
title = "A Pilot Study for {C}hinese {SQL} Semantic Parsing",
author = "Min, Qingkai  and
Shi, Yuefeng  and
Zhang, Yue",
booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
month = nov,
year = "2019",
address = "Hong Kong, China",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/D19-1377",
doi = "10.18653/v1/D19-1377",
pages = "3652--3658",
abstract = "The task of semantic parsing is highly useful for dialogue and question answering systems. Many datasets have been proposed to map natural language text into SQL, among which the recent Spider dataset provides cross-domain samples with multiple tables and complex queries. We build a Spider dataset for Chinese, which is currently a low-resource language in this task area. Interesting research questions arise from the uniqueness of the language, which requires word segmentation, and also from the fact that SQL keywords and columns of DB tables are typically written in English. We compare character- and word-based encoders for a semantic parser, and different embedding schemes. Results show that word-based semantic parser is subject to segmentation errors and cross-lingual word embeddings are useful for text-to-SQL.",
}
@article{DBLP:journals/corr/abs-2006-06434,
author    = {Ningyuan Sun and
        Xuefeng Yang and
        Yunfeng Liu},
title     = {TableQA: a Large-Scale Chinese Text-to-SQL Dataset for Table-Aware
{SQL} Generation},
journal   = {CoRR},
volume    = {abs/2006.06434},
year      = {2020},
url       = {https://arxiv.org/abs/2006.06434},
eprinttype = {arXiv},
eprint    = {2006.06434},
timestamp = {Sat, 13 Jun 2020 18:28:13 +0200},
biburl    = {https://dblp.org/rec/journals/corr/abs-2006-06434.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{wang-etal-2020-dusql,
title = "{D}u{SQL}: A Large-Scale and Pragmatic {C}hinese Text-to-{SQL} Dataset",
author = "Wang, Lijie  and
Zhang, Ao  and
Wu, Kun  and
Sun, Ke  and
Li, Zhenghua  and
Wu, Hua  and
Zhang, Min  and
Wang, Haifeng",
booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
month = nov,
year = "2020",
address = "Online",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2020.emnlp-main.562",
doi = "10.18653/v1/2020.emnlp-main.562",
pages = "6923--6935",
abstract = "Due to the lack of labeled data, previous research on text-to-SQL parsing mainly focuses on English. Representative English datasets include ATIS, WikiSQL, Spider, etc. This paper presents DuSQL, a larges-scale and pragmatic Chinese dataset for the cross-domain text-to-SQL task, containing 200 databases, 813 tables, and 23,797 question/SQL pairs. Our new dataset has three major characteristics. First, by manually analyzing questions from several representative applications, we try to figure out the true distribution of SQL queries in real-life needs. Second, DuSQL contains a considerable proportion of SQL queries involving row or column calculations, motivated by our analysis on the SQL query distributions. Finally, we adopt an effective data construction framework via human-computer collaboration. The basic idea is automatically generating SQL queries based on the SQL grammar and constrained by the given database. This paper describes in detail the construction process and data statistics of DuSQL. Moreover, we present and compare performance of several open-source text-to-SQL parsers with minor modification to accommodate Chinese, including a simple yet effective extension to IRNet for handling calculation SQL queries.",
}
@article{DBLP:journals/corr/abs-2109-05395,
    author    = {Yongrui Chen and
            Xinnan Guo and
            Chaojie Wang and
            Jian Qiu and
            Guilin Qi and
            Meng Wang and
            Huiying Li},
    title     = {Leveraging Table Content for Zero-shot Text-to-SQL with Meta-Learning},
    journal   = {CoRR},
    volume    = {abs/2109.05395},
    year      = {2021},
    url       = {https://arxiv.org/abs/2109.05395},
    eprinttype = {arXiv},
    eprint    = {2109.05395},
    timestamp = {Fri, 15 Jul 2022 16:46:45 +0200},
    biburl    = {https://dblp.org/rec/journals/corr/abs-2109-05395.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{tuan-nguyen-etal-2020-pilot,
title = "A Pilot Study of Text-to-{SQL} Semantic Parsing for {V}ietnamese",
author = "Tuan Nguyen, Anh  and
Dao, Mai Hoang  and
Nguyen, Dat Quoc",
booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
month = nov,
year = "2020",
address = "Online",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2020.findings-emnlp.364",
doi = "10.18653/v1/2020.findings-emnlp.364",
pages = "4079--4085",
abstract = "Semantic parsing is an important NLP task. However, Vietnamese is a low-resource language in this research area. In this paper, we present the first public large-scale Text-to-SQL semantic parsing dataset for Vietnamese. We extend and evaluate two strong semantic parsing baselines EditSQL (Zhang et al., 2019) and IRNet (Guo et al., 2019) on our dataset. We compare the two baselines with key configurations and find that: automatic Vietnamese word segmentation improves the parsing results of both baselines; the normalized pointwise mutual information (NPMI) score (Bouma, 2009) is useful for schema linking; latent syntactic features extracted from a neural dependency parser for Vietnamese also improve the results; and the monolingual language model PhoBERT for Vietnamese (Nguyen and Nguyen, 2020) helps produce higher performances than the recent best multilingual language model XLM-R (Conneau et al., 2020).",
}
@article{DBLP:journals/corr/abs-2110-03546,
author    = {Marcelo Archanjo Jos{\'{e}} and
F{\'{a}}bio Gagliardi Cozman},
title     = {mRAT-SQL+GAP: {A} Portuguese Text-to-SQL Transformer},
journal   = {CoRR},
volume    = {abs/2110.03546},
year      = {2021},
url       = {https://arxiv.org/abs/2110.03546},
eprinttype = {arXiv},
eprint    = {2110.03546},
timestamp = {Thu, 21 Oct 2021 16:20:08 +0200},
biburl    = {https://dblp.org/rec/journals/corr/abs-2110-03546.bib},
bibsource = {dblp computer science bibliography, https://dblp.org}
}